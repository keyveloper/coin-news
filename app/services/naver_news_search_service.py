from langchain_community.document_loaders import WebBaseLoader
from typing import Optional, List, Dict
from langchain_core.documents import Document
from app.services.naver_news_api_service import NaverNewsAPIClient
from urllib.parse import urlparse
from collections import Counter
import time


class NaverNewsSearchService:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ì„œë¹„ìŠ¤ (Singleton)"""

    _instance: Optional['NaverNewsSearchService'] = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        # Singleton íŒ¨í„´ì—ì„œ ì¤‘ë³µ ì´ˆê¸°í™” ë°©ì§€
        if not hasattr(self, '_initialized'):
            self._initialized = True
            self._api_client = NaverNewsAPIClient()

    def __extract_news_metadata_from_url(self, urls: List[str]) -> List[Document]:
        loader = WebBaseLoader(web_paths=urls)
        docs = loader.load()
        return docs

    def __preprocess_page_content(self, content: str) -> str:
        """
        í˜ì´ì§€ ì½˜í…ì¸  ì „ì²˜ë¦¬

        Args:
            content: ì›ë³¸ í…ìŠ¤íŠ¸

        Returns:
            ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ (ì¤„ë°”ê¿ˆ ì œê±°, ì—°ì† ê³µë°± ì œê±°)
        """
        import re
        # \n ì œê±°
        content = content.replace('\n', ' ')
        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì¹˜í™˜
        content = re.sub(r'\s+', ' ', content)
        # ì–‘ìª½ ê³µë°± ì œê±°
        content = content.strip()
        return content

    def search_news(
        self,
        query: str,
        display: int = 10,
        start: int = 1,
        sort: str = "date"
    ) -> Dict:
        # response = NaverNewsResponse
        response = self._api_client.fetch_news(
            query=query,
            display=display,
            start=start,
            sort=sort
        )

        originallinks = [item.originallink for item in response.items]
        docs = self.__extract_news_metadata_from_url(originallinks)

        # ê° ë¬¸ì„œì˜ page_content ì „ì²˜ë¦¬ ë° URLë³„ ë§¤í•‘
        docs_by_url = {}
        for doc in docs:
            doc.page_content = self.__preprocess_page_content(doc.page_content)
            # metadataì—ì„œ source URL ì¶”ì¶œ
            url = doc.metadata.get('source', '')
            docs_by_url[url] = doc

        # ê° itemì— í•´ë‹¹í•˜ëŠ” doc ì¶”ê°€
        items_with_docs = []
        for item in response.items:
            item_dict = item.model_dump()
            doc = docs_by_url.get(item.originallink)

            if doc:
                item_dict['document'] = {
                    'metadata': doc.metadata,
                    'page_content': doc.page_content
                }
            else:
                item_dict['document'] = None

            items_with_docs.append(item_dict)

        return {
            "status": "success",
            "message": f"'{query}' ê´€ë ¨ ë‰´ìŠ¤ {len(response.items)}ê±´ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.",
            "data": {
                "lastBuildDate": response.lastBuildDate,
                "total": response.total,
                "start": response.start,
                "display": response.display,
                "items": items_with_docs
            }
        }

    def analyze_news_site_distribution(
        self,
        query: str,
        iterations: int,
        display: int
    ) -> Dict:
        """
        ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ë¶„í¬ë„ ë¶„ì„

        Args:
            query: ê²€ìƒ‰ì–´
            iterations: ë°˜ë³µ íšŸìˆ˜ (ê¸°ë³¸ 10ë²ˆ)
            display: í•œ ë²ˆì— ê°€ì ¸ì˜¬ ë‰´ìŠ¤ ê°œìˆ˜ (ê¸°ë³¸ 100ê°œ)

        Returns:
            ë‰´ìŠ¤ ì‚¬ì´íŠ¸ë³„ ë¶„í¬ í†µê³„
        """
        print(f"\n========== ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ë¶„í¬ ë¶„ì„ ì‹œì‘ ==========")
        print(f"ê²€ìƒ‰ì–´: {query}")
        print(f"ë°˜ë³µ íšŸìˆ˜: {iterations}")
        print(f"ë°˜ë³µë‹¹ ë‰´ìŠ¤ ê°œìˆ˜: {display}")
        print(f"ì˜ˆìƒ ì´ ë‰´ìŠ¤ ê°œìˆ˜: {iterations * display}")
        print("=" * 50)

        all_links = []

        # STEP 1: iterationsë²ˆ ë°˜ë³µí•˜ì—¬ ë‰´ìŠ¤ ìˆ˜ì§‘
        print(f"\n[STEP 1] ë‰´ìŠ¤ ë§í¬ ìˆ˜ì§‘ ì‹œì‘ ({iterations}ë²ˆ ë°˜ë³µ)")
        for i in range(iterations):
            # ì‹œì‘ ìœ„ì¹˜ ê³„ì‚° (1, 101, 201, ...)
            start_position = i * display + 1
            print(f"\n  [{i+1}/{iterations}] API í˜¸ì¶œ - start: {start_position}, display: {display}")

            try:
                # Naver News API í˜¸ì¶œ
                response = self._api_client.fetch_news(
                    query=query,
                    display=display,
                    start=start_position,
                    sort="date"
                )
                print(f"  âœ“ API ì‘ë‹µ ì„±ê³µ - ë°›ì€ ë‰´ìŠ¤ ê°œìˆ˜: {len(response.items)}")

                # originallinkë§Œ ì¶”ì¶œ
                links = [item.originallink for item in response.items]
                all_links.extend(links)
                print(f"  âœ“ ë§í¬ ì¶”ì¶œ ì™„ë£Œ - í˜„ì¬ê¹Œì§€ ì´ {len(all_links)}ê°œ")

                # API Rate Limiting ë°©ì§€: ë‹¤ìŒ í˜¸ì¶œ ì „ 3ì´ˆ ëŒ€ê¸° (ë§ˆì§€ë§‰ iteration ì œì™¸)
                if i < iterations - 1:
                    print(f"  â³ API Rate Limiting ë°©ì§€ - 3ì´ˆ ëŒ€ê¸°...")
                    time.sleep(3)

            except Exception as e:
                # ì—ëŸ¬ ë°œìƒ ì‹œ í•´ë‹¹ iteration ê±´ë„ˆë›°ê¸°
                print(f"  âœ— ì—ëŸ¬ ë°œìƒ: {str(e)}")
                print(f"  â†’ í•´ë‹¹ iteration ê±´ë„ˆë›°ê¸°")
                continue

        print(f"\n[STEP 1 ì™„ë£Œ] ì´ ìˆ˜ì§‘ëœ ë§í¬: {len(all_links)}ê°œ")

        # STEP 2: URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ
        print(f"\n[STEP 2] URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ ì‹œì‘")
        domains = []
        for idx, link in enumerate(all_links):
            try:
                parsed = urlparse(link)
                domain = parsed.netloc

                # www. ì œê±°
                if domain.startswith('www.'):
                    domain = domain[4:]

                domains.append(domain)

                # ì§„í–‰ìƒí™© í‘œì‹œ (ë§¤ 100ê°œë§ˆë‹¤)
                if (idx + 1) % 100 == 0:
                    print(f"  ì§„í–‰ì¤‘... {idx + 1}/{len(all_links)} ì²˜ë¦¬ ì™„ë£Œ")

            except Exception as e:
                print(f"  âœ— URL íŒŒì‹± ì‹¤íŒ¨ ({link}): {str(e)}")
                continue

        print(f"[STEP 2 ì™„ë£Œ] ì´ ì¶”ì¶œëœ ë„ë©”ì¸: {len(domains)}ê°œ")

        # STEP 3: ë„ë©”ì¸ë³„ ì¹´ìš´íŠ¸
        print(f"\n[STEP 3] ë„ë©”ì¸ë³„ í†µê³„ ê³„ì‚° ì‹œì‘")
        domain_counter = Counter(domains)
        print(f"[STEP 3 ì™„ë£Œ] ê³ ìœ  ë„ë©”ì¸ ìˆ˜: {len(domain_counter)}ê°œ")

        # STEP 4: í†µê³„ ì •ë³´ ìƒì„±
        print(f"\n[STEP 4] ìµœì¢… í†µê³„ ìƒì„±")
        total_count = len(all_links)
        unique_domains = len(domain_counter)

        # ìƒìœ„ ë„ë©”ì¸ ë¦¬ìŠ¤íŠ¸ (ë§ì€ ìˆœì„œëŒ€ë¡œ)
        top_domains = [
            {
                "domain": domain,
                "count": count,
                "percentage": round((count / total_count * 100), 2)
            }
            for domain, count in domain_counter.most_common()
        ]

        # ìƒìœ„ 5ê°œ ë„ë©”ì¸ ì¶œë ¥
        print(f"\n  ğŸ“Š ìƒìœ„ 5ê°œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸:")
        for i, item in enumerate(top_domains[:5], 1):
            print(f"    {i}. {item['domain']}: {item['count']}ê°œ ({item['percentage']}%)")

        print(f"\n========== ë¶„ì„ ì™„ë£Œ ==========\n")

        return {
            "status": "success",
            "message": f"'{query}' ê²€ìƒ‰ì–´ë¡œ {total_count}ê°œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ë¶„í¬ ë¶„ì„ ì™„ë£Œ",
            "data": {
                "query": query,
                "total_news_count": total_count,
                "unique_domains_count": unique_domains,
                "iterations": iterations,
                "display_per_iteration": display,
                "domain_distribution": top_domains
            }
        }